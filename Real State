
import urllib.request
import time
from bs4 import BeautifulSoup
import smtplib
import pymysql


user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'
headers={'User-Agent':user_agent,} 


class Stack():
    
    def __init__(self):
        self.item={}   
        
    def pop(self):

    	return self.item.popitem()
        
    
    def push(self, n, m):
        
        z={n:m}
        
        self.item.update(z)

    def size(self):

    	return len(self.item)






def unscape(stringing):

	stringing = stringing.replace('&amp;', '&')

	return stringing


def cut(intstring):

	s='href="'       #                                                     This function extracts the urls of individual houses when 
	s2='"><img'      #                                                       given function Get (which is the main page)
	out= intstring[intstring.index(s)+len(s):intstring.index(s2)]

	return out



def Get(url):																# Scraps the main page containing the results of a search

	Buffer=[]

	request=urllib.request.Request(url,None,headers) #The assembled request
	html = urllib.request.urlopen(request)
	bs=BeautifulSoup(html.read(), 'html.parser')
	output=bs.find_all({'a'}, {'data-testid': 'listing-details-image-link'})

	
	for i in output:									#      Gathers the links of the houses and adds them into the list 
														#        that it then returns.
			
		Buffer.append(cut(str(i)))

	
	
	return Buffer			                                  # returns a list with the URLs




def Borough(place):

	secHouses=[]
	mainHouses=[]

	print('loading')

	def recurPages(page):

		print('.')

		secHouses=Get(page)
		for i in secHouses: mainHouses.append(i)
		secHouses=[]

		request=urllib.request.Request(page, None, headers)
		html=urllib.request.urlopen(request)
		bs=BeautifulSoup(html.read(), 'html.parser')

		select=bs. find_all('a', {'class':'css-7lz865-StyledPaginationLink eaoxhri5'})
		BNext=select[len(select)-1]  
		StNext=str(BNext)   				#We equal BNext to the Next button and turn it into string


		condition1=str(BNext.get_text())									   	    # we create two conditions to check if we are or not in the last page 
		condition2=StNext[StNext.index('aria-disabled="')+15:StNext.index('aria-disabled="')+20]                            #   and if we are looking at the right place   

		try:
			NextPageUrl='****'+StNext[StNext.index('href="')+6:StNext.index('">Next')]	         #  <--- **** == Cut1
		except:
			NextPageUrl=''

		if 'Next' in condition1 and 'false' in condition2:

			recurPages(unscape(NextPageUrl))


		return unscape(NextPageUrl)


	recurPages(place)
	
	return mainHouses		#it returns a list with the url of the houses in that borough // after iterating through pages
							#         the format of the pages is:     -f{}/.........    ->>> need to concat to website





	


def Dive(link):
																			#  Crawls the links gathered from the search in Get
	Info=[]
																			#   and extracts the desired information
	request=urllib.request.Request(link, None, headers)
	html=urllib.request.urlopen(request)
	bs=BeautifulSoup(html.read(), 'html.parser')
	output=bs.find('p', {'class':'ui-pricing__main-price ui-text-t4'})

	return str(output.get_text())

	#class="ui-pricing__main-price ui-text-t4"













# Here we will code the implementation



#  Hard coding the 32 Boroughs

Cdn=''
Grwch=''
Hck=''
Hmmth=''
Isl=''
KChels=''
Lbth=''
Lwshm=''
Sthwk=''
TowH=''
Wdswth=''
Wmst=''
BarkD=''
Barnt=''
Bxly=''
Brent=''
Bromly=''
Croy=''
Eal=''
Enfd=''
Hgey=''
Hrow=''
Hvg=''
Hldn=''
Hslw=''
KuT=''
Mton=''
Nwham=''
Rbg=''
RuT=''
Sutt=''
Wfor=''


Bghs={'Candem':Cdn, 'Greenwich':Grwch, 'Hackney':Hck, 'Hammersmith':Hmmth, 'Islington':Isl, 'Kensington&Chelsea':KChels, 'Lambeth':Lbth, 'Lewisham':Lwshm, 'Southwark':Sthwk, 'TowerHamlets':TowH}


Have=Borough('URL')				# <---- URL1


print(Dive('URL')) 				 # Here just to check manually and visually <---- URL2

print(Have)
#print(Get(Have))


